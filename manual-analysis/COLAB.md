# Guia Completo: Usando `run.ipynb` no Google Colab

Este guia detalha como usar o notebook `run.ipynb` para an√°lise interativa de padr√µes de projeto usando modelos LLM no Google Colab.

## üìã Vis√£o Geral

O notebook `run.ipynb` √© uma interface otimizada para Google Colab que permite:
- Carregar modelos LLM especializados em c√≥digo (DeepSeek Coder, Phi-2, etc.)
- Fazer an√°lise interativa de padr√µes de projeto
- Salvar conversas automaticamente com timestamp
- Baixar conversas salvas diretamente do Colab

## üéØ Pr√©-requisitos

1. **Conta Google** com acesso ao Google Colab
2. **GPU Runtime** (recomendado): Para modelos maiores como DeepSeek Coder 6.7B
   - No Colab: Runtime ‚Üí Change runtime type ‚Üí GPU (T4 ou superior)
   - GPU gratuita dispon√≠vel, mas com limita√ß√µes de tempo

## üöÄ Passo a Passo Detalhado

### Passo 1: Abrir o Notebook no Colab

1. Acesse o Google Colab: https://colab.research.google.com/
2. Fa√ßa upload do arquivo `run.ipynb` ou clone o reposit√≥rio:
   ```python
   !git clone https://github.com/josiasdeveloper/software-engineering.git
   %cd software-engineering/manual-analysis
   ```

### Passo 2: Configurar o Modelo (C√©lula 1)

**O que faz**: Define qual modelo LLM ser√° usado para an√°lise.

```python
import os

# Op√ß√£o 1: DeepSeek Coder (recomendado - melhor qualidade)
MODEL = "deepseek-ai/deepseek-coder-6.7b-instruct"

# Op√ß√£o 2: Phi-2 (menor, mais r√°pido, menos preciso)
# MODEL = "microsoft/phi-2"

os.environ['LLM_MODEL'] = MODEL
print(f"Model configured: {MODEL}")
```

**Escolha do modelo**:
- **DeepSeek Coder 6.7B**: Melhor para an√°lise detalhada, requer ~15GB GPU
- **Phi-2**: Mais r√°pido, menor mem√≥ria, √∫til para testes r√°pidos

**Dica**: Se o modelo n√£o carregar por falta de mem√≥ria, use `microsoft/phi-2`.

### Passo 3: Clonar/Atualizar Reposit√≥rio (C√©lula 2)

**O que faz**: Garante que voc√™ tem o c√≥digo mais recente do reposit√≥rio.

```python
import os

REPO_URL = "https://github.com/josiasdeveloper/software-engineering.git"

if os.path.exists('software-engineering'):
    print("Repository exists, updating...")
    %cd software-engineering/manual-analysis
    !git pull
else:
    print("Cloning repository...")
    !git clone {REPO_URL}
    %cd software-engineering/manual-analysis

print("\nReady!")
```

**Comportamento**:
- Se o reposit√≥rio j√° existe, atualiza (`git pull`)
- Se n√£o existe, clona do zero
- Sempre navega para `manual-analysis/` ao final

**Nota**: Se voc√™ j√° est√° no diret√≥rio correto, pode pular esta c√©lula.

### Passo 4: Instalar Depend√™ncias (C√©lula 3)

**O que faz**: Instala o pacote `manual-analysis` em modo desenvolvimento.

```python
!pip install -e . -q
```

**O que acontece**:
- Instala todas as depend√™ncias listadas em `requirements.txt`
- Instala o pacote em modo edit√°vel (`-e`)
- Flag `-q` reduz verbosidade

**Tempo estimado**: 1-2 minutos na primeira execu√ß√£o.

**Troubleshooting**:
- Se der erro, tente: `!pip install -e . --no-cache-dir`
- Verifique se est√° no diret√≥rio correto: `!pwd`

### Passo 5: Executar Chat Interativo (C√©lula 4)

**O que faz**: Inicia o chat interativo com o modelo LLM.

```python
!chat
```

**O que acontece**:
1. Carrega o modelo LLM (pode levar 1-3 minutos na primeira vez)
2. Mostra banner de boas-vindas
3. Inicia loop interativo de chat

**Primeira execu√ß√£o**: O modelo ser√° baixado do Hugging Face (~13GB para DeepSeek Coder).

## üí¨ Comandos Dispon√≠veis no Chat

Durante o chat, voc√™ pode usar os seguintes comandos:

### `/help`
Mostra ajuda completa com todos os comandos e dicas de uso.

**Uso**: Digite `/help` e pressione Enter.

### `/clear`
Limpa o hist√≥rico de conversa, mas mant√©m o modelo carregado.

**Quando usar**: 
- Quando quiser come√ßar uma nova an√°lise sem perder o modelo carregado
- Para liberar contexto sem recarregar o modelo

**Uso**: Digite `/clear` e pressione Enter.

### `/restart`
Reinicia o modelo completamente e limpa todo o contexto.

**Quando usar**:
- Quando o modelo est√° com comportamento estranho
- Para liberar mem√≥ria GPU completamente
- Ap√≥s mudar configura√ß√µes

**Uso**: Digite `/restart` e pressione Enter.

**Nota**: O modelo ser√° recarregado na pr√≥xima mensagem (pode demorar).

### `/save`
Salva a conversa atual em um arquivo com timestamp autom√°tico.

**Formato do arquivo**: `conversation_YYYYMMDD_HHMMSS.txt`

**Quando usar**:
- Ao final de uma sess√£o de an√°lise
- Antes de fazer `/restart` ou `/clear`
- Para manter registro das respostas do modelo

**Uso**: Digite `/save` e pressione Enter.

**Exemplo de sa√≠da**:
```
Conversation saved to: conversation_51215_143022.txt
```

### `/context`
Mostra informa√ß√µes sobre o uso atual do contexto.

**Informa√ß√µes exibidas**:
- Porcentagem de tokens usados (com barra visual)
- N√∫mero atual de tokens
- N√∫mero m√°ximo de tokens
- Quantidade de mensagens
- Se h√° resumo ativo

**Quando usar**:
- Para verificar se est√° pr√≥ximo do limite
- Para entender quando o resumo autom√°tico ser√° ativado

**Uso**: Digite `/context` e pressione Enter.

**Exemplo de sa√≠da**:
```
Current Context Usage
Token Usage: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 45.2%
- Current: 7,234 tokens
- Maximum: 16,000 tokens

Messages: 8
Has Summary: No
```

### `/raw`
Mostra a √∫ltima resposta do assistente em texto puro (sem formata√ß√£o).

**Quando usar**:
- Para copiar facilmente a resposta
- Quando precisa do texto sem markdown
- Para colar em outros documentos

**Uso**: Digite `/raw` e pressione Enter.

### `/exit`
Sai do chat interativo.

**Uso**: Digite `/exit` e pressione Enter.

**Nota**: O modelo permanece carregado na mem√≥ria at√© reiniciar o runtime.

## üìù Modos de Entrada

### Entrada Simples (Uma Linha)
Simplesmente digite sua pergunta e pressione Enter:

```
You: What design patterns are used in this code?
```

### Entrada Multi-linha (C√≥digo)
Para colar c√≥digo ou fazer perguntas longas:

1. Digite `begin` e pressione Enter
2. Cole ou digite seu c√≥digo (m√∫ltiplas linhas)
3. Digite `end` em uma nova linha e pressione Enter

**Exemplo**:
```
You: begin
class MyClass:
    def __init__(self):
        self.value = 10
end
```

**Dica**: Voc√™ tamb√©m pode simplesmente colar c√≥digo diretamente - o sistema detecta m√∫ltiplas linhas automaticamente.

## üéØ Exemplos de Uso

### Exemplo 1: An√°lise de Padr√£o Strategy

```
You: begin
# src/vanna/core/llm/base.py
from abc import ABC, abstractmethod

class LlmService(ABC):
    @abstractmethod
    async def send_request(self, request: LlmRequest) -> LlmResponse:
        pass

# src/vanna/integrations/anthropic/llm.py
class AnthropicLlmService(LlmService):
    async def send_request(self, request: LlmRequest) -> LlmResponse:
        # Implementa√ß√£o espec√≠fica do Anthropic
        pass

# src/vanna/integrations/openai/llm.py
class OpenAILlmService(LlmService):
    async def send_request(self, request: LlmRequest) -> LlmResponse:
        # Implementa√ß√£o espec√≠fica do OpenAI
        pass
end

You: Estas duas classes implementam qual padr√£o de projeto em rela√ß√£o √† interface abstrata anterior?
```

### Exemplo 2: Pergunta Direta

```
You: Como o Template Method Pattern √© usado na classe Tool?
```

### Exemplo 3: An√°lise com Contexto

```
You: Analise este c√≥digo e identifique padr√µes de projeto:
[cole o c√≥digo aqui]

You: Como este padr√£o facilita a extensibilidade?
```

## üì• Passo 6: Baixar Conversas Salvas (C√©lula 5)

**O que faz**: Localiza e baixa todos os arquivos de conversa salvos.

```python
import os
import glob

# Encontra todos os arquivos de conversa
conversation_files = glob.glob('conversation_*.txt')

if not conversation_files:
    print("No conversation files found.")
    print("Make sure to use /save command in the chat first.")
else:
    print(f"Found {len(conversation_files)} conversation file(s):")
    for f in conversation_files:
        print(f"  - {f}")
    
    # Tenta baixar se estiver no Colab
    try:
        from google.colab import files
        print("\nDownloading files...")
        for f in conversation_files:
            files.download(f)
        print("\nAll conversations downloaded!")
    except ImportError:
        print("\nNot running in Google Colab.")
        print("Files are saved in the current directory.")
```

**Comportamento**:
- Lista todos os arquivos `conversation_*.txt` no diret√≥rio atual
- Se estiver no Colab, baixa automaticamente cada arquivo
- Se n√£o estiver no Colab, apenas mostra os arquivos encontrados

**Quando usar**:
- Ao final de uma sess√£o de an√°lise
- Antes de desconectar do Colab
- Para fazer backup das conversas

**Dica**: Execute esta c√©lula antes de desconectar do runtime para n√£o perder as conversas.

## üîß Configura√ß√µes Avan√ßadas

### Alterar Modelo Durante Execu√ß√£o

Voc√™ pode mudar o modelo editando `src/manual/config.py`:

```python
MODEL_NAME = "microsoft/phi-2"  # Mude aqui
MAX_CONTEXT_TOKENS = 16000
SUMMARIZE_THRESHOLD = 0.7
```

Depois, use `/restart` no chat para recarregar.

### Ajustar Limites de Contexto

Edite `src/manual/config.py`:

```python
MAX_CONTEXT_TOKENS = 32000  # Aumenta para modelos maiores
SUMMARIZE_THRESHOLD = 0.8   # Resumir quando usar 80% do contexto
```

### Resumo Autom√°tico

O sistema automaticamente resume conversas longas quando:
- O uso de tokens excede `SUMMARIZE_THRESHOLD` (padr√£o: 70%)
- H√° mais de `MAX_HISTORY_BEFORE_SUMMARY` mensagens (padr√£o: 10)

Isso preserva o contexto importante enquanto libera espa√ßo.

## ‚ö†Ô∏è Troubleshooting

### Problema: Modelo n√£o carrega / Erro de mem√≥ria

**Solu√ß√£o 1**: Use um modelo menor
```python
MODEL = "microsoft/phi-2"
```

**Solu√ß√£o 2**: Reinicie o runtime
- Runtime ‚Üí Restart runtime
- Execute novamente as c√©lulas

**Solu√ß√£o 3**: Verifique o tipo de GPU
- Runtime ‚Üí Change runtime type
- Selecione GPU (T4 ou superior)

### Problema: Chat n√£o responde / Trava

**Solu√ß√£o**: Use `/restart` para reiniciar o modelo.

### Problema: Conversas n√£o s√£o salvas

**Verifique**:
1. Voc√™ usou `/save` no chat?
2. Est√° no diret√≥rio correto? (`manual-analysis/`)
3. Permiss√µes de escrita no Colab

**Solu√ß√£o**: Execute `!pwd` para verificar o diret√≥rio atual.

### Problema: Depend√™ncias n√£o instalam

**Solu√ß√£o**:
```python
!pip install --upgrade pip
!pip install -e . --no-cache-dir
```

### Problema: Git clone falha

**Solu√ß√£o**: Verifique conex√£o com internet:
```python
!ping -c 3 github.com
```

## üí° Dicas e Boas Pr√°ticas

### 1. Salve Frequentemente
Use `/save` regularmente para n√£o perder trabalho se o runtime desconectar.

### 2. Monitore o Contexto
Use `/context` periodicamente para evitar surpresas com resumo autom√°tico.

### 3. Use `/raw` para Copiar
Quando precisar copiar respostas, use `/raw` para obter texto limpo.

### 4. Organize Suas Perguntas
Fa√ßa perguntas espec√≠ficas e bem estruturadas para melhores respostas.

### 5. Use Multi-linha para C√≥digo
Para c√≥digo longo, use o modo `begin`/`end` para melhor formata√ß√£o.

### 6. Baixe Antes de Desconectar
Sempre execute a c√©lula de download antes de desconectar do Colab.

### 7. Limite de Tempo do Colab
- Runtime gratuito: ~12 horas cont√≠nuas
- Desconex√£o autom√°tica ap√≥s inatividade
- Salve e baixe conversas regularmente

## üìä Fluxo de Trabalho Recomendado

1. **Configurar**: Execute c√©lulas 1-3 (configura√ß√£o, clone, instala√ß√£o)
2. **Iniciar Chat**: Execute c√©lula 4 (`!chat`)
3. **An√°lise**: Fa√ßa suas perguntas e an√°lises
4. **Salvar**: Use `/save` periodicamente
5. **Baixar**: Execute c√©lula 5 antes de desconectar
6. **Documentar**: Use as conversas salvas para documenta√ß√£o

## üîó Recursos Adicionais

- **README.md**: Documenta√ß√£o geral da ferramenta
- **src/manual/config.py**: Todas as configura√ß√µes dispon√≠veis
- **src/manual/command.py**: C√≥digo fonte dos comandos
- **documentation/DESIGN_PATTERNS_INSUMOS.md**: Exemplos de an√°lises realizadas

## üìÑ Estrutura dos Arquivos de Conversa

Os arquivos salvos com `/save` cont√™m:
- Timestamp da conversa
- Todas as mensagens do usu√°rio
- Todas as respostas do assistente
- Formata√ß√£o markdown preservada

**Formato**:
```
=== Conversation History ===
Timestamp: 2025-12-15 14:30:22

[User]: Pergunta do usu√°rio
[Assistant]: Resposta do modelo

[User]: Pr√≥xima pergunta
[Assistant]: Pr√≥xima resposta
...
```

## üéì Casos de Uso

### An√°lise de Padr√µes de Projeto
1. Cole snippets de c√≥digo
2. Pergunte sobre padr√µes espec√≠ficos
3. Pe√ßa explica√ß√µes detalhadas
4. Valide identifica√ß√µes

### Valida√ß√£o Cruzada
1. Analise o mesmo c√≥digo com diferentes modelos
2. Compare respostas
3. Identifique consensos e diverg√™ncias

### Documenta√ß√£o
1. Use `/raw` para copiar respostas
2. Organize em documentos estruturados
3. Referencie √≠ndices `X.Y.z` do sistema de documenta√ß√£o

## üìù Notas Finais

- O notebook √© otimizado para Google Colab, mas pode funcionar localmente
- Modelos grandes requerem GPU - use runtime com GPU habilitada
- Salve frequentemente - runtime pode desconectar ap√≥s inatividade
- Use `/context` para monitorar uso de recursos
- Respostas s√£o geradas em markdown - use `/raw` para texto puro

---

**√öltima atualiza√ß√£o**: Dezembro 2025
**Vers√£o**: 1.0

